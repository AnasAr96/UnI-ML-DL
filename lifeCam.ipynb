{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Run the following cells to see the live detection of facial emotions"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# When you are using the CNN Model without \"Disgust\" , you have to choose the right   class_names Array"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 22:26:31.216919: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-05-16 22:26:31.218529: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\"]\n",
    "#class_names = [\"angry\",\"disgust\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprised\"]\n",
    "class_names = [\"angry\",\"fear\",\"happy\",\"neutral\",\"sad\",\"surprised\"]\n",
    "classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"]\n",
    "\n",
    "model = tf.keras.models.load_model('models&Historys/model/CNN_With_Generator_No_Disgust_model.h5')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def closeAll():\n",
    "    cv2.namedWindow(\"win\")\n",
    "    cv2.startWindowThread()\n",
    "    cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 22:26:31.648909: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-05-16 22:26:31.720227: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "#thest the model on one image\n",
    "faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "frame = cv2.imread(\"test.png\")\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "faces = faceCascade.detectMultiScale(gray, 1.1, 4)\n",
    "for x, y, w, h in faces:\n",
    "    roi_gray = gray[y:y + h, x: x + w]\n",
    "    roi_color = frame[y:y + h, x: x + w]\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 11, 55), 2)\n",
    "    facess = faceCascade.detectMultiScale(roi_gray)\n",
    "    if len(facess) == 0:\n",
    "\n",
    "        print(\"Face not detected\")\n",
    "    else:\n",
    "        for (ex, ey, ew, eh) in facess:  #show found faces and the predicted emotions\n",
    "            face_roi = roi_color[ey: ey + eh, ex: ex + ew]\n",
    "            final_image = cv2.resize(face_roi, (160, 160))\n",
    "            final_image = np.expand_dims(final_image, axis=0)  #need 4th Dimention\n",
    "            #final_image = final_image / 255.0  #normalize\n",
    "            predictions = model.predict(final_image)\n",
    "            print(class_names[np.argmax(predictions)])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: out device of bound (0-0): 1\n",
      "OpenCV: camera failed to properly initialize!\n"
     ]
    }
   ],
   "source": [
    "# Run the live cam\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "#set the rectangle background to white\n",
    "rectangle_bgr = (255, 255, 255)\n",
    "#make a black image\n",
    "img = np.zeros((500, 500))\n",
    "#set some text\n",
    "text = \"Some text in a box!\"\n",
    "# get the width and height of the text box\n",
    "(text_width, text_height) = cv2.getTextSize(text, font, fontScale=1.5, thickness=1)[0]\n",
    "# set the text start position\n",
    "text_offset_x = 10\n",
    "text_offset_y = img.shape[0] - 25\n",
    "#make the coords of the box with a small padding of two pixels\n",
    "box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height - 2))\n",
    "cv2.rectangle(img, box_coords[0], box_coords[1], rectangle_bgr, cv2.FILLED)\n",
    "cv2.putText(img, text, (text_offset_x, text_offset_y), font, fontScale=1.5, color=(0, 0, 0), thickness=1)\n",
    "cap = cv2.VideoCapture(1)\n",
    "# Check if the webcam is opened correctly\n",
    "if not cap.isOpened():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    raise IOError(\"Cannot open webcam\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    #eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "    faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #print(faceCascade.empty())\n",
    "    faces = faceCascade.detectMultiScale(gray,1.1,4)\n",
    "    for x,y,w,h in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_color = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        facess = faceCascade.detectMultiScale(roi_gray)\n",
    "        if len(facess) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for (ex,ey,ew,eh) in facess:\n",
    "                face_roi = roi_color[ey: ey+eh, ex:ex + ew] ## cropping the face\n",
    "                final_image = cv2.resize(face_roi, (160,160))\n",
    "                final_image = np.expand_dims(final_image,axis=0) ## need fourth dimension # we dont rescale because the model have a rescaling layer\n",
    "                font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                Predictions = model.predict(final_image)\n",
    "                #font = cv2.FONT_HERSHEY_PLAIN\n",
    "\n",
    "\n",
    "                #show the name of predicted emotion\n",
    "                status = class_names[np.argmax(Predictions)]\n",
    "                x1,y1,w1,h1 = 0,0,175,75\n",
    "                #Draw black background rectangle\n",
    "                cv2.rectangle(frame, (x1, x1), (x1 + w1, y1 + h1), (0,0,0), -1)\n",
    "                #Addd text\n",
    "                cv2.putText(frame, status, (x1 + int(w1/10), y1 + int(h1/2)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,255), 2)\n",
    "                cv2.putText(frame, status,(100,150),font, 3,(0, 0, 255),2,cv2.LINE_4)\n",
    "                cv2.rectangle(frame, (x,y), (x+w, y+h), (0, 0, 255))\n",
    "\n",
    "    cv2.imshow('Face Emotion Recognition', frame)\n",
    "    if cv2.waitKey(2) & 0xFF == ord('q') :\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "closeAll() # if the python3.80 window did not close, use this cell separately"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}